---
title: "Untitled"
author: "Fisher Ankney"
date: "2/28/2018"
output: html_document
---
<br> 

### Introduction

The goal of a model is to provide a simple low-dimensional summary of a dataset. In these chapters we partition data into patterns and residuals. Strong patterns hide subtle trends, so we'll use models to peel back the layers. 

```{r, message=FALSE}
library('tidyverse')
library('modelr')
```

The goal of a model is to provide a summary of a dataset, there are two types of models - prediction model (supervised) and data discovery (unsupervised). 

When modeling, each observation can either be used for exploration or confirmation, but not both. You can only use an observation for confirmation once a good approach is to split the data up into three peices: <br> 
 1. 60% training (exploration) allowed to do anything <br> 
 2. 20% query set, used to compare models and visualizations <br> 
 3. 20% test set, can be used once to test final model <br> 

<br> 

### Simple Model 

Check out sim1, then use a model to attempt to capture the pattern in the data - 
```{r}
sim1

ggplot(sim1, aes(x,y)) + 
  geom_point()
```

<br> 
 
Attempting to model this, create 250 different values for the linear model, y = mx + b. Next, visualize these combinations: 
```{r}
models <- tibble(
  b = runif(250, -20, 40),
  m = runif(250, -5, 5)
) 

ggplot(sim1, aes(x, y)) + 
  geom_abline(aes(intercept = b, slope = m),
             data = models,
             alpha = 1/4) + 
  geom_point()
```

<br> 

There are 250 models on this plot, but a lot are pretty bad. We need to find the good models. An easy place to start is to find the vertical distance between each point and the model, the distance between the prediction and the response. To compute this distance, we need to turn the model family into a function. 

Turning the above model into a function: 
```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

model1(c(7,1.5), sim1)
```

<br> 

Now we need to figure out how to compute an overal distance, or collapse these 30 values into a single meaningful number. 

root-mean-squared deviation
```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}

measure_distance(c(7, 1.5), sim1)
```

<br> 

And now that's done, we can use purrr to compute the distances for all of the models, generalizing our scenario. sim1_dist is a new function that takes two inputs, a1 and a2. The function calls measure_distance, which is the same function as previously described. Now, the previously defined tibble models is called, and dplyr is used to create a new variable, dist. `dist` calls the purr function map2 to loop through the slope and intercept variables previously created in the tibble. map2_dbl runs these variables through the function sim1_dist and saves the result to distance. 
```{r}
sim1_dist <- function(a1,a2) {
  measure_distance(c(a1,a2), sim1)
}

models <- models %>%
  mutate(dist = purrr::map2_dbl(b,m, sim1_dist))
models
```

<br> 

Plotting the 10 best slope and intercept combinations: 
```{r}
ggplot(sim1, aes(x, y)) +
  geom_point(size = 2, color = "grey30") + 
  geom_abline(
    aes(intercept = b, slope = m, color = -dist),
    data = filter(models, rank(dist) <= 10)
    
  )
```

<br> 

You could also visualize the model fits by scatterplot: 
```{r}
ggplot(models, aes(b, m)) + 
    geom_point(data = filter(models, rank(dist) <= 10),
               size = 4,
               color = "red") + 
    geom_point(aes(color = -dist))
```

Instead of a random distribution of model variables, lets try grid searching. Note `expand.grid` creates a data frame from all combinations of the supplied vectors or factors. 

```{r}
grid <- expand.grid(
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)
  ) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))  
```

<br> 

Overlaying these best 10 models on the original data gives you some great fits: 
```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(grid, rank(dist) <= 10)
  )
```

 newton-raphson search using optim()

best <- optim(c(0,0), measure_distance, data = sim1)
best$par

ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, color = "grey30") + 
  geom_abline(intercept = best$par[1], slope = best$par[2])


 fitting linear models 
lm() 

sim1_mod <- lm(y ~ x, data = sim1) 
coef(sim1_mod)

sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)


 mean-absolute distance
measure_distance <- function(mod, data) {
  diff <- data$y - make_prediction(mod, data)
  mean(abs(diff))
}

 understanding models and residuals 

data_grid()  create even grid of values covering region where
             data lies

grid <- sim1 %>% 
  data_grid(x) 
grid

grid <- grid %>% 
  add_predictions(sim1_mod)
grid

 ggplot(sim1, aes(x)) + 
   geom_point(aes(y = y)) + 
   geom_line(aes(y = pred), data = grid, color = "red", size = 1)

 Residuals
  tells you what the model has missed, the distance between
  the predicted and observed value 

add_residuals() 

sim1 <- sim1 %>%
  add_residuals(sim1_mod)
sim1

 understand the spread of the residual 
ggplot(sim1, aes(resid)) +
  geom_freqpoly(binwidth = 0.5)

 average of residual will always be zero 
 often want to recreate plots using the residual

ggplot(sim1, aes(x, resid)) + 
  geom_ref_line(h = 0) + 
  geom_point()

 when the graph looks like random noise the model fits
 the data signal well 

ggplot(sim2) + 
  geom_point(aes(x,y))

mod2 <- lm(y ~ x, data = sim2)

grid <- sim2 %>%
  data_grid(x) %>%
  add_predictions(mod2)
grid

ggplot(sim2, aes(x)) + 
  geom_point(aes(y = y)) + 
  geom_point(data = grid, aes(y = pred), color = "red", size = 4)


 Interactiosn (continous and categorical)

ggplot(sim3, aes(x1, y)) + 
  geom_point(aes(color = x2))

mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)

grid <- sim3 %>%
  data_grid(x1, x2) %>%
  gather_predictions(mod1, mod2)
grid

ggplot(sim3, aes(x1, y, color = x2)) + 
  geom_point() + 
  geom_line(data = grid, aes(y = pred)) + 
  facet_wrap(~ model)


 facet models by x2

sim3 <- sim3 %>% 
  gather_residuals(mod1, mod2)

ggplot(sim3, aes(x1, resid, colour = x2)) + 
  geom_point() + 
  facet_grid(model ~ x2)



 
 
 
 
 




