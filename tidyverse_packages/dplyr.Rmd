---
title: "Unpacking the Tidyverse - dplyr"
author: "Fisher Ankney"
date: "February 18, 2018"
output: md_document
---

<br> 

### Introduction 

This is the third of eight installments of my *Unpacking the Tidyverse* series. Each installment focuses on one of the eight core packages in Hadley Wickham's tidyverse. Instructions given in each post are mainly derived from Hadley's textbook, [R for Data Science](http://r4ds.had.co.nz/), and CRAN package documentation. This installment of *Unpacking the Tidyverse* focuses on the data-wrangling package, dplyr. The previous installment focuses on the readr package, and can be found [here](link). The next installment focuses on the tidyr package, and can be found [here](link). 

When it comes to data wrangling, the best teacher is experience. Only through hours of using dplyr will you become proficient with it. Many professional data scientists report that more than 80% of their time is spent data wrangling, so there is endless opportunity to practice with dplyr! 

dplyr is by far the largest tidyverse package, it contains countless functions to help with all of your data wrangling needs. In this post, I'll focus on the three families of dplyr functions that I find the most useful: the five data-wrangling verbs, six types of data joins, and an assortment of helper functions. 


```{r, message=FALSE}
library('tidyverse')      # includes dplyr
library('nycflights13')   # example dataset 
```

<br> 

##
Join functions - 
inner_join(x,y,by="")
full_join(x,y,by="")
left_join(x,y,by="")
right_join(x,y,by="")
semi_join() - only joings things with matches
anti_join() - opposite of semi join, only joins not matches


assorted helper functions - Useful with select()
starts_with()
ends_with()
contains()
matches()
num_range("x", 1:5)
rename()



Join functions

you can add things together in an arrange
arrange(hflights, depdelay + arrdelay) 

aggregate functions - - - 
min(), max(), mean(), median(), quantile(x,p), sd(), var(), IQR(), 

dplyr aggregate functions

first(x) the first element of vector x
last(x) the last element
nth(x, n) the nth element 
n() the number of rows in the dataframe that summarise() describes
n_distinct() number of unique values in vector x 

joining data with dplyr - - - 

mutating joings- functions that match variables in datasets
filtering joings - extract rows from combinations
best practices, bindrow, bindcol, 
diagnose what can go wrong
case study 

key is a column that occurs in both tables that you want to join
key in the first table = primary, second table = secondary, or foreign key 
primary key uniquely ID each row in first dataset
acceptable for values not to appear at all in secondary key 

primary key has to uniquely id all rows! might be more than 1 column combined

left_join is the basic join

left_join(*primary table*, *secondary table*, by = name of the key to join as chr string)

right_join is kind of useless

inner join returns only the rows from the first that have a match in the second, every row must be present in both dataset

full_join is the most inclusive join, joins everything no matter if it's present or not 

inner_join(x,y,by="")
full_join(x,y,by="")
left_join(x,y,by="")
right_join(x,y,by="")
semi_join() - only joings things with matches
anti_join() - opposite of semi join, only joins not matches

semi_join provides a concise way to filter data from the first dataset based on the second datset. 
- number of albums in albums dataset that were made by a band in the bands dataset - 

albums %>%
  semi_join(bands, by = "band") %>%
  nrow()
  
you really just have to know how the primary and secondary keys work, and which join does what. 

anti_joins do the opposite of semi_join, only returns rows that dont have matches

union() provides an easy way to combine two datasets without duplicating any values 
intersect() set operator equivalent for semi-join, what you would use if your datasets have the exact same variables. 

setdiff() does the same thing as anti_join() but uses every column as a key 


setequal() to check if two datasets contain the same rows in any order, returns T/F

bind_rows() - I really dont understand the .id arguement, useful for turning lists into one large table or dataframe
bind_cols() 

data_frame() 
as_data_frame() - turns lists into dataframe

dplyr doesnt coerce data, except for factors, but gives warning

rownames_to_column(name of dataframe, name of new column to add) - might be useful

also colorRampPalette() can be a pretty fun RColorBrewer Tool  :) 

rename(data, new_name = old_data) - very useful 

you can use select() to rename columns with an equals 

reduce function from purrr

tables <- list(surnames, first, middle) 
reduce(tables, left_join, by = "name")

tally() counts things


### Tidyverse Post Template - 

<br> 

### Introduction 
- This is the x of an 8 part series 
- last post here, next post here 
- most taken from r4ds found here
- more information found here

<br> 

### Primary Functionality
- list of primary functions and what they do 

### Minor Functionality 
- list of minor perks from the package and what they do 

<br> 

### Primary Functions m - n 
- explain and examples

<br> 

### Summary 

<br> 



This is the third installment of a six-part series summarizing the concepts of Hadley Wickham's textbook, [R for Data Science](http://r4ds.had.co.nz/). In the previous blog [post](link) I abridged the book’s chapters that cover importing, parsing, and exporting data with readr.

In this post, I’ll be focusing on chapter five which covers data transformation primarily with the dplyr package. dplyr shares tidyverse center stage with ggplot2; unfortunately, dplyr has a steeper learning curve than its co-star, ggplot2. I've written a post on data visualization with ggplot2 [here](post). When it comes to dplyr and data wrangling, it is a topic best learned through hours of practice and experience rather than careful explanation. As such, I hope to offer an easily understood reference guide in this post, rather than a detailed tutorial.

For more in-depth resources on dplyr, reference these links:

- [RStudio’s Data Wrangling Cheatsheet]( https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)
- [Data School’s dplyr Tutorial Video](https://www.youtube.com/watch?v=jWjqLW-u3hc)
- [CRAN Introduction to dplyr]( https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html)
- [CRAN dplyr Documentation (long)]( https://cran.r-project.org/web/packages/dplyr/dplyr.pdf)

<br  />

#### Libraries
```{r, message=FALSE}
library('nycflights13')   # example dataset 
library('tidyverse')      # includes dplyr
library('dplyr')          # dplyr specifically
```

<br  />
 
Some data scientists assert that over 80% of their time is used wrangling data; see this as an opportunity and important reason to master the art of dplyr. If you're looking to make the most of your data analysis routine, it makes sense to streamline the largest portion of your work-load before seeking efficiency elsewhere. 

I've compiled this list of seven primary commands that form the core of dplyr's functionality. These are the most influential and called upon functions in the dplyr package. Seven second-tier functions follow; these commands are commonly useful but not as influential as the primary functions listed before them. 

<br  />

#### Primary Functions
```{r, eval=FALSE}
filter()       # select rows based on value 
arrange()      # sort rows based on values 
select()       # zoom in on specified columns
mutate()       # create new variables from existing ones 
summarize()    # collapse dataframe to single summary
group_by()     # analyze by specified group, useful for summarize 
%>%            # connecting pipe, read as "then"
```

<br  />

#### Secondary Functions
```{r, eval=FALSE}
transmute()    # create new variables from existing ones, remove existing variables
ungroup()      # literally the name
desc()         # descending order (large to small, z to a)
count()        # simple function counting entries in a variable
n()            # number of entries
lag()          # offset, allows to refer to lagging (-1) value
lead()         # offset, allows to refer to leading (+1) value
```

<br  />

I'll give specific examples for the first seven of these functions, and attempt to include the second seven throughout these examples.  Again, the goal of this post is to be a quick-reference as to the functionality of dplyr, not an in-depth tutorial. The best way to learn data wrangling is experience, [R for Data Science Chapter 5](http://r4ds.had.co.nz/transform.html) has many examples and challenges you can work through to solidify your dplyr skills. 

<br  />

#### Example Dataset 
```{r}
flights
```

<br  />

Flights is a classic dataset that is large and diverse, perfect for testing out the primary functions of dpylr. This dataset gives on-time data for all flights that departed from New York City in 2013. There are 19 columns or variables, and 336,776 rows or observations. 

<br  /> 

#### Filter
```{r}
filter(flights, month == 12 & day == 25)
```

<br  />

*Filter* is a simple function that finds, or 'filters' observations that match true to a declared condition. In this example *filter* first's argument is the flights dataset, the following arguments declare the conditions to be met. Retain observations (rows) with the months variable equal to 12, and the day variable equal to 25. The result is a dataframe with 719 flights that departed New York City on Christmas Day, 2013. 

<br  />

#### Arrange
```{r}
arrange(flights, desc(dep_time))
```

<br  />

Often observations within a dataframe are ordered arbitrarily, or unproductively. *Arrange* reorders observations according to its user specified arguments. In this example, the flights dataframe is called upon as the first argument once again; this is common syntax within dplyr functions and I won't be referencing it henceforth. The second argument is *desc(dep_time)*. *desc* is one of the secondary functions listed previously; it simply transforms a vector into a format that will be sorted in descending order. *dep_time* is the flights variable departure time, a number from 0000 to 2400, indicating the actual departure time of an individual aircraft. The resulting dataframe has all 336,776 observations sorted from latest (highest) to earliest (lowest) departure time. 

<br  /> 

#### Select
```{r}
select(flights, year:day)
```

<br  />

In a dataframe with numerous variables it's easy become overwhelmed and broad with analysis. *Select* reduces the number of variables in a dataframe, only keeping variables that the user inputs as arguments. In this example, flights are reduced from 19 variables to the variables year, month and day. 

<br  />

#### Mutate
```{r}
mutate(flights, speed = distance / air_time * 60) %>%
  select(tailnum, distance, air_time, speed)
```

<br  />

*Mutate* allows users to alter current variables and create new ones through various vectorized functions. In the above example the variable speed is created and is equal to distance divided by air time multiplied by 60. The pipe function %>%, is used to move the *mutate* output to *select*. The new variable, speed, is output by the *select* function, along with each flight's tail number, air time, and travel distance.

<br  />

#### Summarize
```{r}
summarize(flights, delay = mean(dep_delay, na.rm = TRUE))
```

<br  />

*Summarize* is perhaps the most complicated function in dplyr. Often used in conjunction with *group_by*, *summarize* collapses many values into a single summary. In the above example. *summarize* finds the mean departure delay of all 336,776 observations in the flights dataset. As a result, the single value 12.64 (minutes) summarizes the mean departure delay. 

<br  />

#### Group By
```{r}
group_by(flights, dest) %>%                       
  summarize(count = n(),                          
            dist = mean(distance, na.rm = TRUE),  
            delay = mean(arr_delay, na.rm = TRUE) 
            ) 
```

<br  /> 

*Summarize* becomes extremely useful when paired with the final primary dplyr function, *group_by*. *group_by* changes the unit of analysis from the complete dataset to an individual group, thus changing the scope of summarize. In the above example, the flights dataset is grouped by the dest variable, destination. Grouping by destination by itself does nothing to the dataframe, so the then pipe, %>%, is used to push the output to the *summarize* function. Summarize first creates a count variable that is equivalent to the function *n( )*. *n( )* is another one of those secondary dplyr functions that often comes in handy; *n( )* is a function that finds the number of observations in the current group. Next, *summarize* creates a variable named distance based off the mean distance travelled by each observation, as grouped by destination. Finally, *summarize* creates a variable named delay, based off the mean arrival delay each observation experienced, as grouped by destination. The resulting dataframe gives excellent insight into each of the 105 destinations present in the flights dataset. 

<br  />

#### Complex Inquiries
```{r}
denver_xmas_delay <- flights %>%                          # create new dataframe, denver_xmas_delay, then
  select(-tailnum) %>%                                    # select all variables except for tailnum 
  filter(month == 12 & day == 25 & dest == "DEN") %>%     # filter only flights with destination Denver on Christmas
  group_by(carrier) %>%                                   # now group fights by carrier company for summary analysis
  summarize(num_flights = n(),                            # create num_flights variable, equal to the count sorted by carrier
            avg_delay = mean(dep_delay)) %>%              # create avg_delay variable, equal to mean departure delay by carrier
  arrange(desc(avg_delay))                                # arrange these carriers by the new avg_delay variable
denver_xmas_delay                                         # print results
```

<br  />

Join functions - 
inner_join(x,y,by="")
full_join(x,y,by="")
left_join(x,y,by="")
right_join(x,y,by="")
semi_join() - only joings things with matches
anti_join() - opposite of semi join, only joins not matches


assorted helper functions - Useful with select()
starts_with()
ends_with()
contains()
matches()
num_range("x", 1:5)
rename()



Join functions

you can add things together in an arrange
arrange(hflights, depdelay + arrdelay) 

aggregate functions - - - 
min(), max(), mean(), median(), quantile(x,p), sd(), var(), IQR(), 

dplyr aggregate functions

first(x) the first element of vector x
last(x) the last element
nth(x, n) the nth element 
n() the number of rows in the dataframe that summarise() describes
n_distinct() number of unique values in vector x 

joining data with dplyr - - - 

mutating joings- functions that match variables in datasets
filtering joings - extract rows from combinations
best practices, bindrow, bindcol, 
diagnose what can go wrong
case study 

key is a column that occurs in both tables that you want to join
key in the first table = primary, second table = secondary, or foreign key 
primary key uniquely ID each row in first dataset
acceptable for values not to appear at all in secondary key 

primary key has to uniquely id all rows! might be more than 1 column combined

left_join is the basic join

left_join(*primary table*, *secondary table*, by = name of the key to join as chr string)

right_join is kind of useless

inner join returns only the rows from the first that have a match in the second, every row must be present in both dataset

full_join is the most inclusive join, joins everything no matter if it's present or not 

inner_join(x,y,by="")
full_join(x,y,by="")
left_join(x,y,by="")
right_join(x,y,by="")
semi_join() - only joings things with matches
anti_join() - opposite of semi join, only joins not matches

semi_join provides a concise way to filter data from the first dataset based on the second datset. 
- number of albums in albums dataset that were made by a band in the bands dataset - 

albums %>%
  semi_join(bands, by = "band") %>%
  nrow()
  
you really just have to know how the primary and secondary keys work, and which join does what. 

anti_joins do the opposite of semi_join, only returns rows that dont have matches

union() provides an easy way to combine two datasets without duplicating any values 
intersect() set operator equivalent for semi-join, what you would use if your datasets have the exact same variables. 

setdiff() does the same thing as anti_join() but uses every column as a key 


setequal() to check if two datasets contain the same rows in any order, returns T/F

bind_rows() - I really dont understand the .id arguement, useful for turning lists into one large table or dataframe
bind_cols() 

data_frame() 
as_data_frame() - turns lists into dataframe

dplyr doesnt coerce data, except for factors, but gives warning

rownames_to_column(name of dataframe, name of new column to add) - might be useful

also colorRampPalette() can be a pretty fun RColorBrewer Tool  :) 

rename(data, new_name = old_data) - very useful 

you can use select() to rename columns with an equals 

reduce function from purrr

tables <- list(surnames, first, middle) 
reduce(tables, left_join, by = "name")

tally() counts things


### Tidyverse Post Template - 

<br> 

### Introduction 
- This is the x of an 8 part series 
- last post here, next post here 
- most taken from r4ds found here
- more information found here

<br> 

### Primary Functionality
- list of primary functions and what they do 

### Minor Functionality 
- list of minor perks from the package and what they do 

<br> 

### Primary Functions m - n 
- explain and examples

<br> 

### Summary 

<br> 



This is the third installment of a six-part series summarizing the concepts of Hadley Wickham's textbook, [R for Data Science](http://r4ds.had.co.nz/). In the previous blog [post](link) I abridged the book’s chapters that cover importing, parsing, and exporting data with readr.

In this post, I’ll be focusing on chapter five which covers data transformation primarily with the dplyr package. dplyr shares tidyverse center stage with ggplot2; unfortunately, dplyr has a steeper learning curve than its co-star, ggplot2. I've written a post on data visualization with ggplot2 [here](post). When it comes to dplyr and data wrangling, it is a topic best learned through hours of practice and experience rather than careful explanation. As such, I hope to offer an easily understood reference guide in this post, rather than a detailed tutorial.

For more in-depth resources on dplyr, reference these links:

- [RStudio’s Data Wrangling Cheatsheet]( https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)
- [Data School’s dplyr Tutorial Video](https://www.youtube.com/watch?v=jWjqLW-u3hc)
- [CRAN Introduction to dplyr]( https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html)
- [CRAN dplyr Documentation (long)]( https://cran.r-project.org/web/packages/dplyr/dplyr.pdf)

<br  />





These primary dplyr functions are effortlessly strung together with the pipe, %>%, to create a complex and extremely specific inquiry into the dataframe. Dplyr's advantage over base R resides in its readability (largely due to the pipe), as well as it's intuitive use functions. I find it's far easier to build a complex inquiry like the example above, one step at a time. It's far easier after each step instead of saving it for the end. 

Dplyr can often be a frustrating package to work with, but once you've become proficient at data wrangling, Hadley Wickham's package becomes near indispensable. Hopefully this reference guide can help you on your journey to mastering dplyr and data wrangling with R. Stay tuned for part four of the six part R4DS summary series, tidy data with tidyr. 

Until next time, <br  />
\- Fisher

